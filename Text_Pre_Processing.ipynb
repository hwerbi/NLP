{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referece: Natural Language Processing\n",
    "Article  in  INTERNATIONAL JOURNAL OF COMPUTER SCIENCES AND ENGINEERING Â· January 2018\n",
    "\n",
    "1. Syntactic analysis (The job of syntactic analysis module is to decide the starting and ending of each sentence in the input \n",
    " document.)\n",
    "2. Tokenizer (The job of tokenizer is to break the sentence given as the output from the syntactic analysis module into tokens.\n",
    "The broken pieces of a sentence can be words, numbers or punctuation marks.)\n",
    "3. Semantic anlysis (This process of assigning and dividing the word into different classes is called Part-Of-Speech tagging \n",
    " or POS tagging)\n",
    "4. Stop Word Removal (Some words are used more often in the natural language text but their value of importance in extracting \n",
    " meaning is very little in regards when we consider overall meaning of the sentence.)\n",
    "5. Stemming (Stemming is the task of evaluating basic form of a each word in the input text document.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\stef\n",
      "[nltk_data]     info\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi ! my name is Natural Language Pre Processing. And you ?\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "request=input()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi !', 'my name is natural language pre processing.', 'and you ?']\n"
     ]
    }
   ],
   "source": [
    "#1. Syntactic analysis\n",
    "\n",
    "request=request.lower()\n",
    "sentences=nltk.sent_tokenize(request)\n",
    "\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi', '!', 'my', 'name', 'is', 'natural', 'language', 'pre', 'processing', '.', 'and', 'you', '?']\n"
     ]
    }
   ],
   "source": [
    "#2. Tokenizer\n",
    "\n",
    "words=nltk.word_tokenize(request)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['INTJ', 'PUNCT', 'DET', 'NOUN', 'AUX', 'ADJ', 'NOUN', 'ADJ', 'NOUN', 'PUNCT', 'CCONJ', 'PRON', 'PUNCT']\n"
     ]
    }
   ],
   "source": [
    "#3. Semantic anlysis\n",
    "\n",
    "doc=nlp(request)\n",
    "part_of_speech=[]\n",
    "\n",
    "for d in doc:\n",
    "    part_of_speech.append(d.pos_)\n",
    "\n",
    "print(part_of_speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi ! my name is natural language pre processing. and you ?\n",
      "[hi, !, my, name, is, natural, language, pre, processing, ., and, you, ?]\n"
     ]
    }
   ],
   "source": [
    "#4. Stop Word Removal\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "filtred_doc=[]\n",
    "\n",
    "for d in doc:\n",
    "    if d not in stop:\n",
    "        filtred_doc.append(d)\n",
    "\n",
    "print(doc)\n",
    "print(filtred_doc)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi', '!', '-PRON-', 'name', 'be', 'natural', 'language', 'pre', 'processing', '.', 'and', '-PRON-', '?']\n"
     ]
    }
   ],
   "source": [
    "#5. Stemming\n",
    "lemmer=nltk.stem.WordNetLemmatizer()\n",
    "base_form=[]\n",
    "for d in doc:\n",
    "    base_form.append(d.lemma_)\n",
    "print(base_form)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
